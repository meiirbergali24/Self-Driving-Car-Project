{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "%matplotlib inline\n",
    "\n",
    "width = 64\n",
    "height = 64\n",
    "channel = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lidar_model = load_model('car_model_lidar.h5')\n",
    "camera_model = load_model('car_model_camera.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for environment & agent interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnvironmentInteraction:\n",
    "    \n",
    "    def __init__(self, Address, Port):\n",
    "        \n",
    "        # This byte array like delimeter uses to divide input byte socket stream to camera and lidar image\n",
    "        self.byteDelimeter = b'\\xff' * 4 + b'\\x00' * 4 + b'\\xff' * 4 + b'\\x00' * 4 + b'\\xff' * 4\n",
    "        \n",
    "        # The ip address of Unity client, usually set to null\n",
    "        self.address = Address\n",
    "        \n",
    "        # The port to listen\n",
    "        self.port = Port\n",
    "        \n",
    "        # The socket object\n",
    "        self.sock = socket.socket()\n",
    "        \n",
    "        # Init socket object\n",
    "        self.sock.bind((self.address, self.port))\n",
    "            \n",
    "        \n",
    "    def listen(self):\n",
    "        \n",
    "        # Start to listen\n",
    "        self.sock.listen(1)\n",
    "        \n",
    "        # Store connection when get request\n",
    "        self.connection, self.address = self.sock.accept()\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        # Send the action to environment\n",
    "        self.connection.send(action.encode('utf-8'))                        # Send action to environment\n",
    "        \n",
    "        # Receive the next images\n",
    "        data = self.connection.recv(768432)                                 # Get camera and lidar images as byte array\n",
    "        \n",
    "        # Split to two byte arrays\n",
    "        data = data.split(self.byteDelimeter)\n",
    "        \n",
    "        # Save the byte arrays as images\n",
    "        camera = Image.frombytes('RGB', (64, 64), data[0])\n",
    "        lidar = Image.frombytes('RGB', (64, 64), data[1])\n",
    "        \n",
    "        # Convert to numpy ndarray with 64x64x3 shape to flip images\n",
    "        camera = np.array(camera.getdata(), dtype = np.float32).reshape(width, height, channel)\n",
    "        lidar = np.array(lidar.getdata(), dtype = np.float32).reshape(width, height, channel)\n",
    "        \n",
    "        # Normalize the data\n",
    "        camera = camera / 255\n",
    "        lidar = lidar / 255\n",
    "        \n",
    "        # Flip the images horizontally\n",
    "        camera = np.flip(camera, axis = 0)    \n",
    "        lidar = np.flip(lidar, axis = 0)\n",
    "        \n",
    "        # Convert to numpy ndarray with 1x64x64x3 shape to predict by models\n",
    "        camera = np.array(camera, dtype = np.float32).reshape(1, width, height, channel)\n",
    "        lidar = np.array(lidar, dtype = np.float32).reshape(1, width, height, channel)\n",
    "                \n",
    "        return camera, lidar\n",
    "\n",
    "# Object of EnvironmentInteraction class\n",
    "environment = EnvironmentInteraction('', 7777) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lidar_target_names = ['Human', 'Clear path', 'Obstacle']\n",
    "lane_target_names = ['Left Lane', 'Right Lane']\n",
    "counter = 0\n",
    "obstacleDetected = False\n",
    "\n",
    "# Function for formatting the command to environment\n",
    "# The control of car in the environment makes by only one number, angle of turning\n",
    "def command(lane_output, angle_output, lidar_output):\n",
    "    \n",
    "    global lidar_target_names\n",
    "    global lane_target_names\n",
    "    global counter\n",
    "    global obstacleDetected\n",
    "    \n",
    "    action = '0'\n",
    "    \n",
    "    \n",
    "    if counter == 15:\n",
    "        counter = 0\n",
    "        obstacleDetected = False\n",
    "    \n",
    "    # This condition is uses to give the car time to go over the next lane \n",
    "    if ((obstacleDetected) & (lidar_target_names[lidar_output] != 'Obstacle')):\n",
    "        counter += 1\n",
    "        return '0'\n",
    "    \n",
    "    # The main priority gives to lidar model, rather than camera model\n",
    "    # Depending on what object is detected, system will make decision how to control the car\n",
    "    if lidar_target_names[lidar_output] == 'Human':\n",
    "        action = '-999'     # Stop moving\n",
    "    elif lidar_target_names[lidar_output] == 'Obstacle':\n",
    "        obstacleDetected = True\n",
    "        if lane_target_names[lane_output] == 'Left Lane':\n",
    "            action = '45'   # move to right lane\n",
    "        else:\n",
    "            action = '-45'  # move to left lane\n",
    "    else:\n",
    "        action = str(angle_output)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.listen()\n",
    "print(environment.connection)\n",
    "\n",
    "action = '0#_#_'\n",
    "\n",
    "while(True):\n",
    "        \n",
    "    # Send the action and get next images to predict\n",
    "    cameraImage, lidarImage = environment.step(action)\n",
    "    \n",
    "    # Make predictions\n",
    "    camera_output = camera_model.predict(cameraImage)\n",
    "    lidar_output = lidar_model.predict(lidarImage)\n",
    "    \n",
    "    # Calculate turning angle\n",
    "    angle = camera_output[1][0][0] * 60\n",
    "    \n",
    "    # Form a new command\n",
    "    action = command(np.argmax(camera_output[0]), angle, np.argmax(lidar_output)) + '#' + lane_target_names[np.argmax(camera_output[0])] + '#' + lidar_target_names[np.argmax(lidar_output)]\n",
    "    \n",
    "    print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
